% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/dataframe__frame.R
\name{DataFrame_write_parquet}
\alias{DataFrame_write_parquet}
\title{Write to parquet file}
\usage{
DataFrame_write_parquet(
  file,
  ...,
  compression = "zstd",
  compression_level = 3,
  statistics = TRUE,
  row_group_size = NULL,
  data_page_size = NULL,
  partition_by = NULL,
  partition_chunk_size_bytes = 4294967296
)
}
\arguments{
\item{file}{File path to which the result should be written. This should be
a path to a directory if writing a partitioned dataset.}

\item{...}{Ignored.}

\item{compression}{String. The compression method. One of:
\itemize{
\item "lz4": fast compression/decompression.
\item "uncompressed"
\item "snappy": this guarantees that the parquet file will be compatible with
older parquet readers.
\item "gzip"
\item "lzo"
\item "brotli"
\item "zstd": good compression performance.
}}

\item{compression_level}{\code{NULL} or Integer. The level of compression to use.
Only used if method is one of 'gzip', 'brotli', or 'zstd'. Higher compression
means smaller files on disk:
\itemize{
\item "gzip": min-level: 0, max-level: 10.
\item "brotli": min-level: 0, max-level: 11.
\item "zstd": min-level: 1, max-level: 22.
}}

\item{statistics}{Whether statistics should be written to the Parquet
headers. Possible values:
\itemize{
\item \code{TRUE}: enable default set of statistics (default)
\item \code{FALSE}: disable all statistics
\item \code{"full"}: calculate and write all available statistics.
\item A named list where all values must be \code{TRUE} or \code{FALSE}, e.g.
\code{list(min = TRUE, max = FALSE)}. Statistics available are \code{"min"}, \code{"max"},
\code{"distinct_count"}, \code{"null_count"}.
}}

\item{row_group_size}{\code{NULL} or Integer. Size of the row groups in number of
rows. If \code{NULL} (default), the chunks of the DataFrame are used. Writing in
smaller chunks may reduce memory pressure and improve writing speeds.}

\item{data_page_size}{Size of the data page in bytes. If \code{NULL} (default), it
is set to 1024^2 bytes.
will be ~1MB.}

\item{partition_by}{Column(s) to partition by. A partitioned dataset will be
written if this is specified.}

\item{partition_chunk_size_bytes}{Approximate size to split DataFrames within
a single partition when writing. Note this is calculated using the size of
the DataFrame in memory - the size of the output file may differ depending
on the file format / compression.}
}
\value{
Invisibly returns the input DataFrame.
}
\description{
Write to parquet file
}
\examples{
\dontshow{if (requireNamespace("withr", quietly = TRUE)) (if (getRversion() >= "3.4") withAutoprint else force)(\{ # examplesIf}
dat = pl$DataFrame(mtcars)

# write data to a single parquet file
destination = withr::local_tempfile(fileext = ".parquet")
dat$write_parquet(destination)

# write data to folder with a hive-partitioned structure
dest_folder = withr::local_tempdir()
dat$write_parquet(dest_folder, partition_by = c("gear", "cyl"))
list.files(dest_folder, recursive = TRUE)
\dontshow{\}) # examplesIf}
}
