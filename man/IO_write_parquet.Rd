% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/dataframe__frame.R
\name{DataFrame_write_parquet}
\alias{DataFrame_write_parquet}
\title{Write to parquet file}
\usage{
DataFrame_write_parquet(
  file,
  ...,
  compression = "zstd",
  compression_level = 3,
  statistics = FALSE,
  row_group_size = NULL,
  data_pagesize_limit = NULL
)
}
\arguments{
\item{file}{File path to which the result should be written.}

\item{...}{Ignored.}

\item{compression}{String. The compression method. One of:
\itemize{
\item "lz4": fast compression/decompression.
\item "uncompressed"
\item "snappy": this guarantees that the parquet file will be compatible with
older parquet readers.
\item "gzip"
\item "lzo"
\item "brotli"
\item "zstd": good compression performance.
}}

\item{compression_level}{\code{NULL} or Integer. The level of compression to use.
Only used if method is one of 'gzip', 'brotli', or 'zstd'. Higher compression
means smaller files on disk:
\itemize{
\item "gzip": min-level: 0, max-level: 10.
\item "brotli": min-level: 0, max-level: 11.
\item "zstd": min-level: 1, max-level: 22.
}}

\item{statistics}{Logical. Whether compute and write column statistics.
This requires extra compute.}

\item{row_group_size}{\code{NULL} or Integer. Size of the row groups in number of
rows. If \code{NULL} (default), the chunks of the DataFrame are used. Writing in
smaller chunks may reduce memory pressure and improve writing speeds.}

\item{data_pagesize_limit}{\code{NULL} or Integer. If \code{NULL} (default), the limit
will be ~1MB.}
}
\value{
Invisibly returns the input DataFrame.
}
\description{
Write to parquet file
}
\examples{
# write table 'mtcars' from mem to parquet
dat = pl$DataFrame(mtcars)

destination = tempfile(fileext = ".parquet")
dat$write_parquet(destination)
}
