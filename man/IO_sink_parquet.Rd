% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/lazyframe__lazy.R
\name{LazyFrame_sink_parquet}
\alias{LazyFrame_sink_parquet}
\title{Stream the output of a query to a Parquet file}
\usage{
LazyFrame_sink_parquet(
  path,
  compression = "zstd",
  compression_level = 3,
  statistics = FALSE,
  row_group_size = NULL,
  data_pagesize_limit = NULL,
  maintain_order = TRUE,
  type_coercion = TRUE,
  predicate_pushdown = TRUE,
  projection_pushdown = TRUE,
  simplify_expression = TRUE,
  slice_pushdown = TRUE,
  no_optimization = FALSE,
  inherit_optimization = FALSE
)
}
\arguments{
\item{path}{String. The path of the parquet file}

\item{compression}{String. The compression method. One of:
\itemize{
\item "lz4": fast compression/decompression.
\item "uncompressed"
\item "snappy": this guarantees that the parquet file will be compatible with
older parquet readers.
\item "gzip"
\item "lzo"
\item "brotli"
\item "zstd": good compression performance.
}}

\item{compression_level}{\code{NULL} or Integer. The level of compression to use.
Only used if method is one of 'gzip', 'brotli', or 'zstd'. Higher compression
means smaller files on disk:
\itemize{
\item "gzip": min-level: 0, max-level: 10.
\item "brotli": min-level: 0, max-level: 11.
\item "zstd": min-level: 1, max-level: 22.
}}

\item{statistics}{Boolean. Whether compute and write column statistics.
This requires extra compute.}

\item{row_group_size}{\code{NULL} or Integer. Size of the row groups in number of
rows. If \code{NULL} (default), the chunks of the DataFrame are used. Writing in
smaller chunks may reduce memory pressure and improve writing speeds.}

\item{data_pagesize_limit}{\code{NULL} or Integer. If \code{NULL} (default), the limit
will be ~1MB.}

\item{maintain_order}{Maintain the order in which data is processed. Setting
this to \code{FALSE} will be slightly faster.}

\item{type_coercion}{Boolean. Coerce types such that operations succeed and
run on minimal required memory.}

\item{predicate_pushdown}{Boolean. Applies filters as early as possible at
scan level.}

\item{projection_pushdown}{Boolean. Select only the columns that are needed
at the scan level.}

\item{simplify_expression}{Boolean. Various optimizations, such as constant
folding and replacing expensive operations with faster alternatives.}

\item{slice_pushdown}{Boolean. Only load the required slice from the scan
level. Don't materialize sliced outputs (e.g. \code{join$head(10)}).}

\item{no_optimization}{Boolean. Sets the following parameters to \code{FALSE}:
\code{predicate_pushdown}, \code{projection_pushdown}, \code{slice_pushdown},
\code{comm_subplan_elim}, \code{comm_subexpr_elim}.}

\item{inherit_optimization}{Boolean. Use existing optimization settings
regardless the settings specified in this function call.}
}
\description{
This writes the output of a query directly to a Parquet file without collecting
it in the R session first. This is useful if the output of the query is still
larger than RAM as it would crash the R session if it was collected into R.
}
\examples{
# sink table 'mtcars' from mem to parquet
tmpf = tempfile()
pl$LazyFrame(mtcars)$sink_parquet(tmpf)

# stream a query end-to-end
tmpf2 = tempfile()
pl$scan_parquet(tmpf)$select(pl$col("cyl") * 2)$sink_parquet(tmpf2)

# load parquet directly into a DataFrame / memory
pl$scan_parquet(tmpf2)$collect()
}
