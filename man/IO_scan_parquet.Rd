% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/io_parquet.R
\name{pl_scan_parquet}
\alias{pl_scan_parquet}
\title{Scan a parquet file}
\usage{
pl_scan_parquet(
  source,
  ...,
  n_rows = NULL,
  row_index_name = NULL,
  row_index_offset = 0L,
  parallel = c("auto", "columns", "row_groups", "none"),
  hive_partitioning = TRUE,
  rechunk = FALSE,
  low_memory = FALSE,
  storage_options = NULL,
  use_statistics = TRUE,
  cache = TRUE
)
}
\arguments{
\item{source}{Path to a file. You can use globbing with \code{*} to scan/read multiple
files in the same directory (see examples).}

\item{...}{Ignored.}

\item{n_rows}{Maximum number of rows to read.}

\item{row_index_name}{If not \code{NULL}, this will insert a row index column with
the given name into the DataFrame.}

\item{row_index_offset}{Offset to start the row index column (only used if
the name is set).}

\item{parallel}{This determines the direction of parallelism. \code{"auto"} will
try to determine the optimal direction. Can be \code{"auto"}, \code{"columns"},
\code{"row_groups"}, or \code{"none"}.}

\item{hive_partitioning}{Infer statistics and schema from hive partitioned URL
and use them to prune reads.}

\item{rechunk}{In case of reading multiple files via a glob pattern, rechunk
the final DataFrame into contiguous memory chunks.}

\item{low_memory}{Reduce memory usage (will yield a lower performance).}

\item{storage_options}{Experimental. List of options necessary to scan
parquet files from different cloud storage providers (GCP, AWS, Azure).
See the 'Details' section.}

\item{use_statistics}{Use statistics in the parquet file to determine if pages
can be skipped from reading.}

\item{cache}{Cache the result after reading.}
}
\value{
\link[=LazyFrame_class]{LazyFrame}
}
\description{
Scan a parquet file
}
\details{
\subsection{Connecting to cloud providers}{

Polars supports scanning parquet files from different cloud providers.
The cloud providers currently supported are AWS, GCP, and Azure.
The supported keys to pass to the \code{storage_options} argument can be found
here:
\itemize{
\item \href{https://docs.rs/object_store/latest/object_store/aws/enum.AmazonS3ConfigKey.html}{aws}
\item \href{https://docs.rs/object_store/latest/object_store/gcp/enum.GoogleConfigKey.html}{gcp}
\item \href{https://docs.rs/object_store/latest/object_store/azure/enum.AzureConfigKey.html}{azure}
}
\subsection{Implementation details}{
\itemize{
\item Currently it is impossible to scan public parquet files from GCP without
a valid service account. Be sure to always include a service account in the
\code{storage_options} argument.
}
}

}
}
\examples{
\dontshow{if (requireNamespace("arrow", quietly = TRUE) && arrow::arrow_with_dataset() && arrow::arrow_with_parquet()) (if (getRversion() >= "3.4") withAutoprint else force)(\{ # examplesIf}
temp_dir = tempfile()
# Write a hive-style partitioned parquet dataset
arrow::write_dataset(
  mtcars,
  temp_dir,
  partitioning = c("cyl", "gear"),
  format = "parquet",
  hive_style = TRUE
)
list.files(temp_dir, recursive = TRUE)

# Read the dataset
pl$scan_parquet(
  file.path(temp_dir, "**/*.parquet")
)$collect()
\dontshow{\}) # examplesIf}
}
