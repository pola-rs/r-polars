% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/io_csv.R
\name{pl_read_csv}
\alias{pl_read_csv}
\title{New DataFrame from CSV}
\usage{
pl_read_csv(
  source,
  ...,
  has_header = TRUE,
  separator = ",",
  comment_prefix = NULL,
  quote_char = "\\"",
  skip_rows = 0,
  dtypes = NULL,
  null_values = NULL,
  ignore_errors = FALSE,
  cache = FALSE,
  infer_schema_length = 100,
  n_rows = NULL,
  encoding = "utf8",
  low_memory = FALSE,
  rechunk = TRUE,
  skip_rows_after_header = 0,
  row_index_name = NULL,
  row_index_offset = 0,
  try_parse_dates = FALSE,
  eol_char = "\\n",
  raise_if_empty = TRUE,
  truncate_ragged_lines = FALSE,
  reuse_downloaded = TRUE
)
}
\arguments{
\item{source}{Path to a file or URL. It is possible to provide multiple paths
provided that all CSV files have the same schema. It is not possible to
provide several URLs.}

\item{...}{Ignored.}

\item{has_header}{Indicate if the first row of dataset is a header or not.If
\code{FALSE}, column names will be autogenerated in the following format: \code{"column_x"}
\code{x} being an enumeration over every column in the dataset starting at 1.}

\item{separator}{Single byte character to use as separator in the file.}

\item{comment_prefix}{A string, which can be up to 5 symbols in length, used to indicate
the start of a comment line. For instance, it can be set to \verb{#} or \verb{//}.}

\item{quote_char}{Single byte character used for quoting. Set to \code{NULL} to
turn off special handling and escaping of quotes.}

\item{skip_rows}{Start reading after a particular number of rows. The header
will be parsed at this offset.}

\item{dtypes}{Named list of column names - dtypes or dtype - column names. This
list is used while reading to overwrite dtypes. Supported types so far are:
\itemize{
\item "Boolean" or "logical" for DataType::Boolean,
\item "Categorical" or "factor" for DataType::Categorical,
\item "Float32" or "double" for DataType::Float32,
\item "Float64" or "float64" for DataType::Float64,
\item "Int32" or "integer" for DataType::Int32,
\item "Int64" or "integer64" for DataType::Int64,
\item "String" or "character" for DataType::String,
}}

\item{null_values}{Values to interpret as \code{NA} values. Can be:
\itemize{
\item a character vector: all values that match one of the values in this vector
will be \code{NA};
\item a named list with column names and null values.
}}

\item{ignore_errors}{Keep reading the file even if some lines yield errors.
You can also use \code{infer_schema_length = 0} to read all columns as UTF8 to
check which values might cause an issue.}

\item{cache}{Cache the result after reading.}

\item{infer_schema_length}{Maximum number of rows to read to infer the column
types. If set to 0, all columns will be read as UTF-8. If \code{NULL}, a full
table scan will be done (slow).}

\item{n_rows}{Maximum number of rows to read.}

\item{encoding}{Either \code{"utf8"} or \code{"utf8-lossy"}. Lossy means that invalid
UTF8 values are replaced with "?" characters.}

\item{low_memory}{Reduce memory usage (will yield a lower performance).}

\item{rechunk}{Reallocate to contiguous memory when all chunks / files are
parsed.}

\item{skip_rows_after_header}{Parse the first row as headers, and then skip
this number of rows.}

\item{row_index_name}{If not \code{NULL}, this will insert a row index column with
the given name into the DataFrame.}

\item{row_index_offset}{Offset to start the row index column (only used if
the name is set).}

\item{try_parse_dates}{Try to automatically parse dates. Most ISO8601-like
formats can be inferred, as well as a handful of others. If this does not
succeed, the column remains of data type \code{pl$String}.}

\item{eol_char}{Single byte end of line character (default: \verb{\\n}). When
encountering a file with Windows line endings (\verb{\\r\\n}), one can go with the
default \verb{\\n}. The extra \verb{\\r} will be removed when processed.}

\item{raise_if_empty}{If \code{FALSE}, parsing an empty file returns an empty
DataFrame or LazyFrame.}

\item{truncate_ragged_lines}{Truncate lines that are longer than the schema.}

\item{reuse_downloaded}{If \code{TRUE}(default) and a URL was provided, cache the
downloaded files in session for an easy reuse.}
}
\value{
DataFrame
}
\description{
New DataFrame from CSV
}
