% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/io-json-functions.R
\name{pl__read_ndjson}
\alias{pl__read_ndjson}
\title{Read into a DataFrame from NDJSON file}
\usage{
pl__read_ndjson(
  source,
  ...,
  schema = NULL,
  schema_overrides = NULL,
  infer_schema_length = 100,
  batch_size = 1024,
  n_rows = NULL,
  low_memory = FALSE,
  rechunk = FALSE,
  row_index_name = NULL,
  row_index_offset = 0L,
  ignore_errors = FALSE,
  storage_options = NULL,
  retries = 2,
  file_cache_ttl = NULL,
  include_file_paths = NULL
)
}
\arguments{
\item{source}{Path(s) to a file or directory. When needing to authenticate
for scanning cloud locations, see the \code{storage_options} parameter.}

\item{...}{These dots are for future extensions and must be empty.}

\item{schema}{Specify the datatypes of the columns. The datatypes must match
the datatypes in the file(s). If there are extra columns that are not in the
file(s), consider also enabling \code{allow_missing_columns}.}

\item{schema_overrides}{Overwrite dtypes during inference. This must be a
list. Names of list elements are used to match to inferred columns.}

\item{infer_schema_length}{The maximum number of rows to scan for schema
inference. If \code{NULL}, the full data may be scanned (this is slow). Set
\code{infer_schema = FALSE} to read all columns as \code{pl$String}.}

\item{n_rows}{Stop reading from parquet file after reading \code{n_rows}.}

\item{low_memory}{Reduce memory pressure at the expense of performance}

\item{rechunk}{In case of reading multiple files via a glob pattern rechunk
the final DataFrame into contiguous memory chunks.}

\item{row_index_name}{If not \code{NULL}, this will insert a row index column with
the given name into the DataFrame.}

\item{row_index_offset}{Offset to start the row index column (only used if
the name is set).}

\item{ignore_errors}{Keep reading the file even if some lines yield errors.
You can also use \code{infer_schema = FALSE} to read all columns as UTF8 to
check which values might cause an issue.}

\item{storage_options}{Named vector containing options that indicate how to
connect to a cloud provider. The cloud providers currently supported are
AWS, GCP, and Azure.
See supported keys here:
\itemize{
\item \href{https://docs.rs/object_store/latest/object_store/aws/enum.AmazonS3ConfigKey.html}{aws}
\item \href{https://docs.rs/object_store/latest/object_store/gcp/enum.GoogleConfigKey.html}{gcp}
\item \href{https://docs.rs/object_store/latest/object_store/azure/enum.AzureConfigKey.html}{azure}
\item Hugging Face (\verb{hf://}): Accepts an API key under the token parameter
\code{c(token = YOUR_TOKEN)} or by setting the \code{HF_TOKEN} environment
variable.
}

If \code{storage_options} is not provided, Polars will try to infer the
information from environment variables.}

\item{retries}{Number of retries if accessing a cloud instance fails.}

\item{file_cache_ttl}{Amount of time to keep downloaded cloud files since
their last access time, in seconds. Uses the \code{POLARS_FILE_CACHE_TTL}
environment variable (which defaults to 1 hour) if not given.}

\item{include_file_paths}{Character value indicating the column name that
will include the path of the source file(s).}
}
\value{
A polars \link{DataFrame}
}
\description{
Read into a DataFrame from NDJSON file
}
\examples{
\dontshow{if (requireNamespace("jsonlite", quietly = TRUE)) (if (getRversion() >= "3.4") withAutoprint else force)(\{ # examplesIf}
ndjson_filename <- tempfile()
jsonlite::stream_out(iris, file(ndjson_filename), verbose = FALSE)
pl$read_ndjson(ndjson_filename)
\dontshow{\}) # examplesIf}
}
