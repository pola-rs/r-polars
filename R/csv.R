#' New LazyFrame from CSV
#'
#' @description
#' Read a file from path into a polars LazyFrame.
#' @name scan_csv
#' @rdname IO_scan_csv
#'
#' @param path Path to a file or URL. It is possible to provide multiple paths
#' provided that all CSV files have the same schema. It is not possible to
#' provide several URLs.
#' @param has_header Indicate if the first row of dataset is a header or not.If
#' `FALSE`, column names will be autogenerated in the following format: `"column_x"`
#' `x` being an enumeration over every column in the dataset starting at 1.
#' @param separator Single byte character to use as separator in the file.
#' @param comment_char Single byte character that indicates the start of a comment
#' line, for instance `#`.
#' @param quote_char Single byte character used for quoting. Set to `NULL` to
#' turn off special handling and escaping of quotes.
#' @param skip_rows Start reading after a particular number of rows. The header
#' will be parsed at this offset.
#' @param dtypes Named list of column names - dtypes or dtype - column names. This
#' list is used while reading to overwrite dtypes. Supported types so far are:
#' * "Boolean" or "logical" for DataType::Boolean,
#' * "Categorical" or "factor" for DataType::Categorical,
#' * "Float32" or "double" for DataType::Float32,
#' * "Float64" or "float64" for DataType::Float64,
#' * "Int32" or "integer" for DataType::Int32,
#' * "Int64" or "integer64" for DataType::Int64,
#' * "Utf8" or "character" for DataType::Utf8,
#' @param null_values Values to interpret as `NA` values. Can be:
#' * a character vector: all values that match one of the values in this vector
#'   will be `NA`;
#' * a named list with column names and null values.
#' @param missing_utf8_is_empty_string By default, a missing value is considered
#' to be `NA`. Setting this parameter to `TRUE` will consider missing UTF8 values
#' as an empty character.
#' @param ignore_errors Keep reading the file even if some lines yield errors.
#' You can also use `infer_schema_length = 0` to read all columns as UTF8 to
#' check which values might cause an issue.
#' @param cache Cache the result after reading.
#' @param infer_schema_length Maximum number of rows to read to infer the column
#' types. If set to 0, all columns will be read as UTF-8. If `NULL`, a full
#' table scan will be done (slow).
#' @param n_rows Maximum number of rows to read.
#' @param encoding Either `"utf8"` or `"utf8-lossy"`. Lossy means that invalid
#' UTF8 values are replaced with "?" characters.
#' @param low_memory Reduce memory usage (will yield a lower performance).
#' @param rechunk Reallocate to contiguous memory when all chunks / files are
#' parsed.
#' @param skip_rows_after_header Parse the first row as headers, and then skip
#' this number of rows.
#' @param try_parse_dates Try to automatically parse dates. Most ISO8601-like
#' formats can be inferred, as well as a handful of others. If this does not
#' succeed, the column remains of data type `pl$Utf8`.
#' @param eol_char Single byte end of line character (default: `\n`). When
#' encountering a file with Windows line endings (`\r\n`), one can go with the
#' default `\n`. The extra `\r` will be removed when processed.
#' @param raise_if_empty If `FALSE`, parsing an empty file returns an empty
#' DataFrame or LazyFrame.
#' @param truncate_ragged_lines Truncate lines that are longer than the schema.
#' @param reuse_downloaded If `TRUE`(default) and a URL was provided, cache the
#' downloaded files in session for an easy reuse.
#' @return scan_csv returns a LazyFrame. read_csv returns a DataFrame.
#' @examples
#' my_file = tempfile()
#' write.csv(iris, my_file)
#' lazy_frame = pl$scan_csv(path = my_file)
#' lazy_frame$collect()
#' unlink(my_file)
pl$scan_csv = function(
    path,
    has_header = TRUE,
    separator = ",",
    comment_char = NULL,
    quote_char = '"',
    skip_rows = 0,
    dtypes = NULL,
    null_values = NULL,
    missing_utf8_is_empty_string = FALSE,
    ignore_errors = FALSE,
    cache = FALSE,
    infer_schema_length = 100,
    n_rows = NULL,
    encoding = "utf8",
    low_memory = FALSE,
    rechunk = TRUE,
    skip_rows_after_header = 0,
    # row_count_name = NULL,
    # row_count_offset = 0,
    try_parse_dates = FALSE,
    eol_char = "\n",
    raise_if_empty = TRUE,
    truncate_ragged_lines = FALSE,
    reuse_downloaded = TRUE) {

  # capture all args and modify some to match lower level function
  args = as.list(environment())

  # single path and vector of paths are handled separately on the Rust side
  if (length(path) > 1) {
    args = append(args, list(paths = path), after = 1)
    args["path"] = list(NULL)
  } else {
    args[["path"]] = check_is_link(args[["path"]], reuse_downloaded = reuse_downloaded)
    args = append(args, list(paths = NULL), after = 1)
  }
  args[["reuse_downloaded"]] = NULL

  # dtypes: convert named list of DataType's to DataTypeVector obj
  if (!is.null(args$dtypes)) {
    args$dtypes = list_to_datatype_vector(args$dtypes)
  }

  # null_values: convert string or un/named  char vec into RNullValues obj
  if (!is.null(args$null_values)) {
    nullvals = args$null_values
    ## TODO support also unnamed list, like will be interpreted as positional dtypes args by polars.
    RNullValues = (function() {
      # one string is used as one NULL marker for all columns
      if (is_string(nullvals)) {
        return(RNullValues$new_all_columns(nullvals))
      }

      # many unnamed strings(char vec) is used one mark for each column
      if (is.character(nullvals) && !is_named(nullvals)) {
        return(RNullValues = RNullValues$new_columns(nullvals))
      }

      # named char vec is used as column(name) marker(value) pairs
      if (is.list(nullvals) && is_named(nullvals)) {
        return(RNullValues$new_named(unlist(null_values)))
      }

      stopf("null_values arg must be a string OR unamed char vec OR named char vec")
    })()

    args$null_values = RNullValues
  }

  ## call low level function with args
  check_no_missing_args(new_from_csv, args)
  unwrap(do.call(new_from_csv, args))
}

#' New DataFrame from CSV
#'
#' @description
#' Read a file from path into a polars DataFrame.
#' @name read_csv
#' @rdname IO_read_csv
#' @param path Path to a file or URL. It is possible to provide multiple paths
#' provided that all CSV files have the same schema. It is not possible to
#' provide several URLs.
#' @param has_header Indicate if the first row of dataset is a header or not.If
#' `FALSE`, column names will be autogenerated in the following format: `"column_x"`
#' `x` being an enumeration over every column in the dataset starting at 1.
#' @param separator Single byte character to use as separator in the file.
#' @param comment_char Single byte character that indicates the start of a comment
#' line, for instance `#`.
#' @param quote_char Single byte character used for quoting. Set to `NULL` to
#' turn off special handling and escaping of quotes.
#' @param skip_rows Start reading after a particular number of rows. The header
#' will be parsed at this offset.
#' @param dtypes Named list of column names - dtypes or dtype - column names. This
#' list is used while reading to overwrite dtypes. Supported types so far are:
#' * "Boolean" or "logical" for DataType::Boolean,
#' * "Categorical" or "factor" for DataType::Categorical,
#' * "Float32" or "double" for DataType::Float32,
#' * "Float64" or "float64" for DataType::Float64,
#' * "Int32" or "integer" for DataType::Int32,
#' * "Int64" or "integer64" for DataType::Int64,
#' * "Utf8" or "character" for DataType::Utf8,
#' @param null_values Values to interpret as `NA` values. Can be:
#' * a character vector: all values that match one of the values in this vector
#'   will be `NA`;
#' * a named list with column names and null values.
#' @param missing_utf8_is_empty_string By default, a missing value is considered
#' to be `NA`. Setting this parameter to `TRUE` will consider missing UTF8 values
#' as an empty character.
#' @param ignore_errors Keep reading the file even if some lines yield errors.
#' You can also use `infer_schema_length = 0` to read all columns as UTF8 to
#' check which values might cause an issue.
#' @param cache Cache the result after reading.
#' @param infer_schema_length Maximum number of rows to read to infer the column
#' types. If set to 0, all columns will be read as UTF-8. If `NULL`, a full
#' table scan will be done (slow).
#' @param n_rows Maximum number of rows to read.
#' @param encoding Either `"utf8"` or `"utf8-lossy"`. Lossy means that invalid
#' UTF8 values are replaced with "?" characters.
#' @param low_memory Reduce memory usage (will yield a lower performance).
#' @param rechunk Reallocate to contiguous memory when all chunks / files are
#' parsed.
#' @param skip_rows_after_header Parse the first row as headers, and then skip
#' this number of rows.
#' @param try_parse_dates Try to automatically parse dates. Most ISO8601-like
#' formats can be inferred, as well as a handful of others. If this does not
#' succeed, the column remains of data type `pl$Utf8`.
#' @param eol_char Single byte end of line character (default: `\n`). When
#' encountering a file with Windows line endings (`\r\n`), one can go with the
#' default `\n`. The extra `\r` will be removed when processed.
#' @param raise_if_empty If `FALSE`, parsing an empty file returns an empty
#' DataFrame or LazyFrame.
#' @param truncate_ragged_lines Truncate lines that are longer than the schema.
#' @param reuse_downloaded If `TRUE`(default) and a URL was provided, cache the
#' downloaded files in session for an easy reuse.
#' @return DataFrame
pl$read_csv = function(
    path,
    has_header = TRUE,
    separator = ",",
    comment_char = NULL,
    quote_char = '"',
    skip_rows = 0,
    dtypes = NULL,
    null_values = NULL,
    missing_utf8_is_empty_string = FALSE,
    ignore_errors = FALSE,
    cache = FALSE,
    infer_schema_length = 100,
    n_rows = NULL,
    encoding = "utf8",
    low_memory = FALSE,
    rechunk = TRUE,
    skip_rows_after_header = 0,
    # row_count_name = NULL,
    # row_count_offset = 0,
    try_parse_dates = FALSE,
    eol_char = "\n",
    raise_if_empty = TRUE,
    truncate_ragged_lines = FALSE,
    reuse_downloaded = TRUE) {
  mc = match.call()
  mc[[1]] = get("pl", envir = asNamespace("polars"))$scan_csv
  eval.parent(mc)$collect()
}


check_is_link = function(path, reuse_downloaded) {
  if (!file.exists(path)) {
    con = NULL

    # check if possible to open url connection
    assumed_schemas = c("", "https://", "http://", "ftp://")
    for (i_schema in assumed_schemas) {
      if (!is.null(con)) break
      actual_url = paste0(i_schema, path)
      suppressWarnings(
        tryCatch(
          {
            con = url(actual_url, open = "rt")
          },
          error = function(e) {}
        )
      )
    }

    # try download file if valid url
    if (!is.null(con)) {
      close(con)
      tmp_file = paste0(tempdir(), "/", make.names(actual_url))
      if (isFALSE(reuse_downloaded) || isFALSE(file.exists(tmp_file))) {
        download.file(url = actual_url, destfile = tmp_file)
        message(paste("tmp file placed in \n", tmp_file))
      }

      path = tmp_file # redirect path to tmp downloaded file
    } else {
      # do nothing let path fail on rust side
      path = NULL
    }
  }

  path
}


list_to_datatype_vector = function(x) {
  if (!is.list(x) || !is_named(x)) {
    stopf("could not interpret dtypes, must be a named list of DataTypes")
  }
  datatype_vector = DataTypeVector$new() # mutable
  mapply(
    name = names(x),
    type = unname(x),
    FUN = function(name, type) {
      # convert possible string to datatype
      if (is_string(type)) {
        type = DataType$new(type)
      }
      if (!inherits(type, "RPolarsDataType")) {
        stopf("arg dtypes must be a named list of dtypes or dtype names")
      }
      datatype_vector$push(name, type)
    }
  )
  datatype_vector
}
